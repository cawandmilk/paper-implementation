{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"[023] Distilling the Knowledge in a Neural Network.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pLifNUJBkZxd"},"source":["# **Distilling the Knowledge in a Neural Network**\n","\n","Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\n","\n","Ref.\n","\n","*https://keras.io/examples/vision/knowledge_distillation/*"]},{"cell_type":"markdown","metadata":{"id":"w9Ql0gM1koEL"},"source":["## **Default Setting**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goZIUUAbkZuw","executionInfo":{"elapsed":6629,"status":"ok","timestamp":1618643828471,"user":{"displayName":"Myung-Gyo Oh","photoUrl":"","userId":"01040732127983096879"},"user_tz":-540},"outputId":"e25af5c0-2375-4150-9cf0-0a10c1a361c2"},"source":["import tensorflow as tf\n","\n","import numpy as np\n","\n","from adabelief_tf import AdaBeliefOptimizer\n","from pathlib import Path\n","\n","print(f\"tf.__version__: {tf.__version__}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.__version__: 2.4.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OThZ-e0elLyb","executionInfo":{"elapsed":5583,"status":"ok","timestamp":1618643828472,"user":{"displayName":"Myung-Gyo Oh","photoUrl":"","userId":"01040732127983096879"},"user_tz":-540},"outputId":"55da0426-ca12-4700-fe16-226a11569974"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tue Apr 20 09:59:51 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.32.00    Driver Version: 455.32.00    CUDA Version: 11.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Graphics Device     On   | 00000000:0A:00.0 Off |                  N/A |\n","|  0%   40C    P8    23W / 220W |     16MiB /  7979MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A       921      G   /usr/lib/xorg/Xorg                  9MiB |\n","|    0   N/A  N/A      1061      G   /usr/bin/gnome-shell                4MiB |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6R4PhjkBlFn9","outputId":"ba447673-b262-4eb0-de3d-629ec1405f74"},"source":["tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n","Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Graphics Device, compute capability 8.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vBuHOtOB_Ytc","outputId":"63be904c-ee36-46cd-f251-3cd145927aab"},"source":["# If you wanna avoid below error, you need to run below codes when you start kernel.\n","\n","# UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, \n","# so try looking to see if a warning log message was printed above. [Op:Conv2D]\n","\n","# Ref: https://blog.naver.com/vft1500/221793591386\n","\n","gpus = tf.config.list_physical_devices(\"GPU\")\n","if gpus:\n","    try:\n","    # Currently, memory growth needs to be the same across GPUs\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        logical_gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n","        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs.\")\n","    except RuntimeError as e:\n","        # Memory growth must be set before GPUs have been initialized\n","        print(e)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 Physical GPUs, 1 Logical GPUs.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ISi8AjJalFq_"},"source":["class HParams(object):\n","    def __init__(self):\n","        self.seed = 42\n","        \n","        self.num_classes = 10\n","        \n","        self.vl_size = 10_000\n","\n","        self.global_batch_size = 256\n","        self.buffer_size = 20_000\n","        self.auto = tf.data.experimental.AUTOTUNE\n","\n","        self.image_size = [112, 112]\n","\n","        self.init_lr = 1e-3\n","        self.epochs = 10\n","        \n","        self.steps_per_epoch = None\n","        self.validation_steps = None\n","        self.steps_per_execution = 16\n","\n","HPARAMS = HParams()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_n1OtrZYnnRw"},"source":["## **Prepare Dataset**"]},{"cell_type":"code","metadata":{"id":"nJYKo-zxnnUm"},"source":["@tf.function\n","def resizing_and_rescaling(images, labels):\n","    images = tf.expand_dims(images, axis = -1)\n","    images = tf.image.convert_image_dtype(images, tf.float32)\n","    images = tf.image.resize(images, HPARAMS.image_size)\n","    # images = tf.image.grayscale_to_rgb(images)\n","    labels = tf.cast(labels, tf.int32)\n","    return images, labels\n","\n","\n","def get_shapes(element_spec):\n","    return [get_shapes(e) if isinstance(e, tuple) else e.shape for e in element_spec]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9aDyBSPnnXj"},"source":["def get_dataset(\n","    batch_size = HPARAMS.global_batch_size\n","):\n","    ## Load dataset from tfds.\n","    (tr_X, tr_Y), (ts_X, ts_Y) = tf.keras.datasets.mnist.load_data()\n","\n","    ## Train test split.\n","    tr_X, vl_X = tr_X[HPARAMS.vl_size:], tr_X[:HPARAMS.vl_size]\n","    tr_Y, vl_Y = tr_Y[HPARAMS.vl_size:], tr_Y[:HPARAMS.vl_size]\n","\n","    ## Building.\n","    tr_ds = tf.data.Dataset.from_tensor_slices((tr_X, tr_Y)\n","                ).cache(\n","                ).repeat(\n","                ).shuffle(HPARAMS.buffer_size, reshuffle_each_iteration = True,\n","                ).batch(batch_size\n","                ).map(resizing_and_rescaling, num_parallel_calls = HPARAMS.auto\n","                ).prefetch(HPARAMS.auto)\n","    \n","    vl_ds = tf.data.Dataset.from_tensor_slices((vl_X, vl_Y)\n","                ).cache(\n","                ).repeat(\n","                # ).shuffle(HPARAMS.buffer_sz, reshuffle_each_iteration = True,\n","                ).batch(batch_size\n","                ).map(resizing_and_rescaling, num_parallel_calls = HPARAMS.auto\n","                ).prefetch(HPARAMS.auto)\n","\n","    ts_ds = tf.data.Dataset.from_tensor_slices((ts_X, ts_Y)\n","                ).cache(\n","                # ).shuffle(HPARAMS.buffer_size, reshuffle_each_iteration = True,\n","                ).batch(batch_size\n","                ).map(resizing_and_rescaling, num_parallel_calls = HPARAMS.auto\n","                ).prefetch(HPARAMS.auto)\n","\n","    steps_per_epoch  = np.ceil(np.shape(tr_X)[0] / batch_size)\n","    validation_steps = np.ceil(np.shape(vl_Y)[0] / batch_size)\n","    \n","    HPARAMS.steps_per_epoch  = steps_per_epoch\n","    HPARAMS.validation_steps = validation_steps\n","    \n","    print(f\"# of training data: {tr_X.shape[0]}\")\n","    print(f\"# of validation data: {vl_X.shape[0]}\")\n","    print(f\"# of test data: {ts_X.shape[0]}\\n\")\n","\n","    print(f\"Global batch size: {batch_size}\")\n","    print(f\"Steps per epoch: {steps_per_epoch} (total {steps_per_epoch * HPARAMS.epochs} batches)\")\n","    print(f\"Validation steps: {validation_steps} (total {validation_steps * HPARAMS.epochs} batches)\\n\")\n","    \n","    print(f\"Steps per execution: {HPARAMS.steps_per_execution}\\n\")\n","\n","    print(f\"tr_ds.element_spec: {get_shapes(tr_ds.element_spec)}\")\n","    print(f\"vl_ds.element_spec: {get_shapes(vl_ds.element_spec)}\")\n","    print(f\"ts_ds.element_spec: {get_shapes(ts_ds.element_spec)}\\n\")\n","\n","    return tr_ds, vl_ds, ts_ds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Zuo7xXP8k9pd"},"source":["## **Modeling**"]},{"cell_type":"markdown","metadata":{"id":"oXbMIf_JoRdP"},"source":["### **Baseline**"]},{"cell_type":"code","metadata":{"id":"gLiEcRtjkZr_"},"source":["def bn_ReLU_conv2D(x, filters, kernel_size):\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = tf.keras.layers.Activation(tf.nn.relu6)(x)\n","    x = tf.keras.layers.Conv2D(filters, kernel_size, padding = \"same\")(x)\n","    \n","    return x\n","\n","\n","def transition_block(x):\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = tf.keras.layers.Conv2D(x.shape[-1] // 2, 1, padding = \"same\")(x)\n","    x = tf.keras.layers.AveragePooling2D((2, 2), strides = 2)(x)\n","\n","    return x\n","\n","\n","def dense_block(x, num_conv, growth_rate):\n","    for i in range(num_conv):\n","        residual = x\n","        x = bn_ReLU_conv2D(x, 4 * growth_rate, 1)\n","        x = bn_ReLU_conv2D(x, growth_rate, 3)\n","        x = tf.keras.layers.Concatenate(axis = -1)([x, residual])\n","\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTlwZ-CU_Ytf"},"source":["def create_NN(\n","    model_name, \n","    growth_rate = 32, \n","    embedding_dim = HPARAMS.num_classes\n","):\n","    ## DenseNet-121\n","    x = model_input = tf.keras.layers.Input(shape = (*HPARAMS.image_size, 1)) ## grayscale, not rgb\n","\n","    ## Entry Flow\n","    x = tf.keras.layers.Conv2D(2 * growth_rate, 7, strides = 2, padding = \"same\")(x)\n","    x = tf.keras.layers.MaxPooling2D((3, 3), strides = 2, padding = \"same\")(x)\n","\n","\n","    ## Middle Flow\n","    for i, num_conv in enumerate([6, 12, 24, 16]):\n","        x = dense_block(x, num_conv, growth_rate)\n","        if i is not 3: \n","            x = transition_block(x)\n","\n","    ## Exit Flow\n","    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","    x = tf.keras.layers.Dense(embedding_dim)(x)\n","    model_output = x = tf.keras.layers.Activation(\"linear\", dtype = tf.float32)(x) ## no classifier!\n","\n","    return tf.keras.Model(\n","        inputs = model_input,\n","        outputs = model_output,\n","        name = model_name\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WHTWgjsGkW8u"},"source":["# tmp = create_NN(\"tmp\")\n","# tmp.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XJHv13Yd_Ytg"},"source":["# del tmp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GEbaaV6ZpsFi"},"source":["### **Loss Function**"]},{"cell_type":"markdown","metadata":{"id":"sDiQX0rCnLFc"},"source":["### **Teacher and Student Model**"]},{"cell_type":"code","metadata":{"id":"KRsTVhII_Ytg"},"source":["class DistillationModelWrapper(tf.keras.Model):\n","    def __init__(\n","        self, \n","        teacher, \n","        student, \n","        **kwargs\n","    ):\n","        super(DistillationModelWrapper, self).__init__(**kwargs)\n","        ## Assert teacher and Student have no classification layer (i.e. softmax).\n","        self.teacher = teacher\n","        self.student = student\n","        \n","    def compile(\n","        self, \n","        optimizer,\n","        student_loss_fn,\n","        distillation_loss_fn,\n","        metrics,\n","        alpha = 0.9,\n","        temperature = 10,\n","        **kwargs,\n","    ):\n","        super(DistillationModelWrapper, self).compile(\n","            optimizer = optimizer,\n","            metrics = metrics,\n","            **kwargs\n","        )\n","        # self.optimizer = optimizer\n","        self.student_loss_fn = student_loss_fn ## sparse categorical crossentropy\n","        self.distillation_loss_fn = distillation_loss_fn ## Kullbackâ€“Leibler divergence\n","        # self.metrics = metrics ## accuracy\n","        self.alpha = alpha\n","        self.temperature = temperature\n","        \n","    @tf.function\n","    def train_step(self, x):\n","        inp, tar = x\n","\n","        teacher_pred = self.teacher(inp, training = False)\n","        \n","        with tf.GradientTape() as tape:\n","            student_pred = self.student(inp)\n","            \n","            ## Calculate losses.\n","            student_loss = self.student_loss_fn(\n","                tar, \n","                student_pred,\n","            )\n","            distillation_loss = self.distillation_loss_fn(\n","                tf.nn.softmax(teacher_pred / self.temperature),\n","                tf.nn.softmax(student_pred / self.temperature),\n","            )\n","            \n","            loss = (1. - self.alpha) * student_loss + self.alpha * distillation_loss\n","            scaled_loss = self.optimizer.get_scaled_loss(loss)\n","            \n","        scaled_grads = tape.gradient(scaled_loss, self.student.trainable_variables)\n","        grads = self.optimizer.get_unscaled_gradients(scaled_grads)\n","        self.optimizer.apply_gradients(zip(grads, self.student.trainable_weights))\n","\n","        self.compiled_metrics.update_state(tar, student_pred)\n","        \n","        results = {m.name: m.result() for m in self.metrics}\n","        results.update({\n","            \"student_loss\": student_loss,\n","            \"distillation_loss\": distillation_loss,\n","        })\n","        \n","        return results\n","    \n","    @tf.function\n","    def test_step(self, x):\n","        inp, tar = x\n","        \n","        teacher_pred = self.teacher(inp, training = False)\n","        student_pred = self.student(inp, training = False)\n","        \n","        student_loss = self.student_loss_fn(\n","            tar, \n","            student_pred,\n","        )\n","        distillation_loss = self.distillation_loss_fn(\n","            tf.nn.softmax(teacher_pred / self.temperature),\n","            tf.nn.softmax(student_pred / self.temperature),\n","        )\n","        \n","        loss = (1. - self.alpha) * student_loss + self.alpha * distillation_loss\n","        \n","        self.compiled_metrics.update_state(tar, student_pred)\n","        \n","        results = {m.name: m.result() for m in self.metrics}\n","        results.update({\"student_loss\": student_loss})\n","        \n","        return results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-oykV2Y_oUhA"},"source":["## **Fit**"]},{"cell_type":"markdown","metadata":{"id":"8SZ-3KQKrwQz"},"source":["### **Callbacks**"]},{"cell_type":"code","metadata":{"id":"MLKZxiE4rwNp"},"source":["def get_callbacks(model_name, is_distiller = False):    \n","    ## Checkpoint callback.\n","    if is_distiller:\n","        ckpt_path = Path(f\"ckpt/{model_name}/\" + \"cp-{epoch:03d}-{val_student_loss:.4f}.ckpt\")\n","        ckpt_path.parent.mkdir(parents = True, exist_ok = True)\n","\n","        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","            ckpt_path, \n","            verbose = 0, \n","            monitor = \"val_student_loss\", \n","            save_weights_only = True, \n","            save_best_only = True,\n","        )\n","    else:\n","        ckpt_path = Path(f\"ckpt/{model_name}/\" + \"cp-{epoch:03d}-{val_loss:.4f}.ckpt\")\n","        ckpt_path.parent.mkdir(parents = True, exist_ok = True)\n","\n","        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n","            ckpt_path, \n","            verbose = 0, \n","            monitor = \"val_loss\", \n","            save_weights_only = True, \n","            save_best_only = True,\n","        )\n","\n","    ## TensorBoard callback.\n","    log_dir = Path(f\"logs/fit/{model_name}\")\n","    tb_callback = tf.keras.callbacks.TensorBoard(\n","        log_dir = log_dir, \n","        histogram_freq = 1,\n","    )\n","\n","    return [cp_callback, tb_callback]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnfH46pO_Yth"},"source":["### **Checkpoints**"]},{"cell_type":"code","metadata":{"id":"LwOgn-ZR_Yti"},"source":["def load_latest_checkpoint(ckpt_folder, growth_rate = 32, is_distiller = False):\n","    latest = tf.train.latest_checkpoint(ckpt_folder)\n","    print(f\"Load latest checkpoints: {latest}...\")\n","    \n","    model_name = ckpt_folder.split(\"/\")[-1]\n","    \n","    if is_distiller:\n","        model = DistillationModelWrapper(\n","            teacher = create_NN(f\"{model_name}-teacher-latest\", growth_rate = 32),\n","            student = create_NN(f\"{model_name}-student-latest\", growth_rate = 12),\n","        )\n","\n","        ckpt = tf.train.Checkpoint(model)\n","        ckpt.restore(latest).expect_partial()\n","        \n","        model.compile(\n","            optimizer = AdaBeliefOptimizer(\n","                learning_rate = HPARAMS.init_lr, \n","                epsilon = 1e-14,\n","                weight_decay = 1e-5,\n","                rectify = True,\n","                print_change_log = False,\n","            ),\n","            student_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(),\n","            distillation_loss_fn = tf.keras.losses.KLDivergence(),\n","            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n","            steps_per_execution = HPARAMS.steps_per_execution,\n","        )\n","    else:\n","        model = create_NN(\n","            model_name = f\"{model_name}-latest\",\n","            growth_rate = growth_rate,\n","        )\n","        \n","        ckpt = tf.train.Checkpoint(model)\n","        ckpt.restore(latest).expect_partial()\n","\n","        model.compile(\n","            optimizer = AdaBeliefOptimizer(\n","                learning_rate = HPARAMS.init_lr, \n","                epsilon = 1e-14,\n","                weight_decay = 1e-5,\n","                rectify = False,\n","                print_change_log = False,\n","            ),\n","            loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n","            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n","            steps_per_execution = HPARAMS.steps_per_execution,\n","        )\n","\n","    print(f\"Restored model: {model.name}\\n\")\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qkC-5_hKq-MS"},"source":["### **Baseline with Small Model**\n","\n","Using the growth_rate parameter defined in DenseNet, it is made thinner than the normal model. The actual number of parameters is about 1M, which is significantly less than the 7M parameters of the large model."]},{"cell_type":"code","metadata":{"id":"GwErs8cZ_Ytj"},"source":["!rm -rf ./ckpt\n","!rm -rf ./logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUAbNiNBoUd1","outputId":"0e5afc60-078d-4fb4-c4c1-8e755f36ba92"},"source":["%%time\n","tr_ds, vl_ds, ts_ds = get_dataset()\n","\n","student = create_NN(\n","    model_name = \"student\",\n","    growth_rate = 12,\n",")\n","\n","print(f\"Training model: {student.name} (# of params: {student.count_params() / 1e+6:.2f}M)\")\n","\n","student.compile(\n","    optimizer = AdaBeliefOptimizer(\n","        learning_rate = HPARAMS.init_lr, \n","        epsilon = 1e-14,\n","        weight_decay = 1e-5,\n","        rectify = True,\n","        print_change_log = False,\n","    ),\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), ## \n","    metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name = \"acc\")],\n","    steps_per_execution = HPARAMS.steps_per_execution,\n",")\n","\n","_ = student.fit(\n","    tr_ds,\n","    validation_data = vl_ds,\n","    steps_per_epoch = HPARAMS.steps_per_epoch,\n","    validation_steps = HPARAMS.validation_steps,\n","    epochs = HPARAMS.epochs,\n","    verbose = 0,\n","    callbacks = get_callbacks(student.name),\n",")\n","\n","print(\"Done.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# of training data: 50000\n","# of validation data: 10000\n","# of test data: 10000\n","\n","Global batch size: 256\n","Steps per epoch: 196.0 (total 1960.0 batches)\n","Validation steps: 40.0 (total 400.0 batches)\n","\n","Steps per execution: 16\n","\n","tr_ds.element_spec: [TensorShape([None, 112, 112, 1]), TensorShape([None])]\n","vl_ds.element_spec: [TensorShape([None, 112, 112, 1]), TensorShape([None])]\n","ts_ds.element_spec: [TensorShape([None, 112, 112, 1]), TensorShape([None])]\n","\n","Training model: student (# of params: 1.03M)\n","Done.\n","CPU times: user 6min 17s, sys: 31.6 s, total: 6min 48s\n","Wall time: 7min 42s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lPlKsCgN_Ytj","outputId":"f9a6ddf4-ede0-4299-90b3-356229fa1bac"},"source":["latest_student = load_latest_checkpoint(f\"./ckpt/{student.name}\", growth_rate = 12)\n","latest_student.evaluate(ts_ds, verbose = 2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load latest checkpoints: ./ckpt/student/cp-004-0.0639.ckpt...\n","Restored model: student-latest\n","\n","40/40 - 3s - loss: 1.2587 - sparse_categorical_accuracy: 0.9811\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[1.2587331533432007, 0.9811000227928162]"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"RvWAdaUH_Ytk"},"source":["### **Baseline with Large Model**"]},{"cell_type":"code","metadata":{"id":"muaYuHW9_Ytk","outputId":"ba74cf31-e876-4938-a7b5-c3e87a7d62e1"},"source":["%%time\n","## Reducing the batch size in half.\n","tr_ds, vl_ds, ts_ds = get_dataset(batch_size = HPARAMS.global_batch_size // 2)\n","\n","teacher = create_NN(\n","    model_name = \"teacher\",\n","    growth_rate = 32,\n",")\n","\n","print(f\"Training model: {teacher.name} (# of params: {teacher.count_params() / 1e+6:.2f}M)\")\n","\n","teacher.compile(\n","    optimizer = AdaBeliefOptimizer(\n","        learning_rate = HPARAMS.init_lr, \n","        epsilon = 1e-14,\n","        weight_decay = 1e-5,\n","        rectify = True,\n","        print_change_log = False,\n","    ),\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True), ##\n","    metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name = \"acc\")],\n","    steps_per_execution = HPARAMS.steps_per_execution,\n",")\n","\n","_ = teacher.fit(\n","    tr_ds,\n","    validation_data = vl_ds,\n","    steps_per_epoch = HPARAMS.steps_per_epoch,\n","    validation_steps = HPARAMS.validation_steps,\n","    epochs = HPARAMS.epochs,\n","    verbose = 0,\n","    callbacks = get_callbacks(teacher.name),\n",")\n","\n","print(\"Done.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# of training data: 50000\n","# of validation data: 10000\n","# of test data: 10000\n","\n","Global batch size: 128\n","Steps per epoch: 391.0 (total 3910.0 batches)\n","Validation steps: 79.0 (total 790.0 batches)\n","\n","Steps per execution: 16\n","\n","tr_ds.element_spec: [TensorShape([None, 112, 112, 1]), TensorShape([None])]\n","vl_ds.element_spec: [TensorShape([None, 112, 112, 1]), TensorShape([None])]\n","ts_ds.element_spec: [TensorShape([None, 112, 112, 1]), TensorShape([None])]\n","\n","Training model: teacher (# of params: 7.05M)\n","Done.\n","CPU times: user 10min 35s, sys: 53.5 s, total: 11min 28s\n","Wall time: 14min 33s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dk0GFvB4_Ytk","outputId":"3e41e22b-b5ad-4ed3-aeda-cea81defbfce"},"source":["latest_teacher = load_latest_checkpoint(f\"./ckpt/{teacher.name}\", growth_rate = 32)\n","latest_teacher.evaluate(ts_ds, verbose = 2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load latest checkpoints: ./ckpt/teacher/cp-005-0.0824.ckpt...\n","Restored model: teacher-latest\n","\n","79/79 - 4s - loss: 1.2586 - sparse_categorical_accuracy: 0.9830\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[1.2585575580596924, 0.9829999804496765]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"RUjkSBkhnLCm"},"source":["### **Distiller Model**\n","\n","With the weights of the latest teacher model fixed, we train a new student model from scratch. By referring to the method introduced in the paper and the keras tutorial, the weight of the soft label was increased (loss weight = 0.9). Specifically, by using kl-divergence as the distiller loss function, the difference between the two entropy probability distributions was calculated."]},{"cell_type":"code","metadata":{"id":"HpZ-vrYQ_Ytl","outputId":"f6cd25dd-06a5-4f4d-e661-1d4af104d8f1"},"source":["## About twice as much VRAM is required, reducing the batch size in half.\n","tr_ds, vl_ds, ts_ds = get_dataset(batch_size = HPARAMS.global_batch_size // 2)\n","\n","latest_teacher.trainable = False ## freeze\n","distiller = DistillationModelWrapper(\n","    teacher = latest_teacher,\n","    student = create_NN(\"distiller-student\", growth_rate = 12),\n","    name = \"distiller\",\n",")\n","\n","print(f\"Training model: {distiller.name}... \") #\"(# of params: {distiller.count_params() / 1e+6:.2f}M)\")\n","\n","distiller.compile(\n","    optimizer = AdaBeliefOptimizer(\n","        learning_rate = HPARAMS.init_lr, \n","        epsilon = 1e-14,\n","        weight_decay = 1e-5,\n","        rectify = True,\n","        print_change_log = False,\n","    ),\n","    student_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n","    distillation_loss_fn = tf.keras.losses.KLDivergence(),\n","    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()],\n","    steps_per_execution = HPARAMS.steps_per_execution,\n",")\n","\n","distiller.fit(\n","    tr_ds,\n","    validation_data = vl_ds,\n","    steps_per_epoch = HPARAMS.steps_per_epoch,\n","    validation_steps = HPARAMS.validation_steps,\n","    epochs = HPARAMS.epochs,\n","    verbose = 2,\n","    callbacks = get_callbacks(distiller.name, is_distiller = True),\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# of training data: 50000\n","# of validation data: 10000\n","# of test data: 10000\n","\n","Global batch size: 128\n","Steps per epoch: 391.0 (total 3910.0 batches)\n","Validation steps: 79.0 (total 790.0 batches)\n","\n","Steps per execution: 16\n","\n","tr_ds.element_spec: [TensorShape([None, 112, 112, 1]), TensorShape([None])]\n","vl_ds.element_spec: [TensorShape([None, 112, 112, 1]), TensorShape([None])]\n","ts_ds.element_spec: [TensorShape([None, 112, 112, 1]), TensorShape([None])]\n","\n","Training model: distiller... \n","Epoch 1/10\n","391/391 - 139s - sparse_categorical_accuracy: 0.7782 - student_loss: 0.2169 - distillation_loss: 0.0370 - val_sparse_categorical_accuracy: 0.9667 - val_student_loss: 0.1647\n","Epoch 2/10\n","391/391 - 64s - sparse_categorical_accuracy: 0.9730 - student_loss: 0.1113 - distillation_loss: 0.0228 - val_sparse_categorical_accuracy: 0.9792 - val_student_loss: 0.0144\n","Epoch 3/10\n","391/391 - 64s - sparse_categorical_accuracy: 0.9847 - student_loss: 0.0489 - distillation_loss: 0.0208 - val_sparse_categorical_accuracy: 0.9780 - val_student_loss: 0.0446\n","Epoch 4/10\n","391/391 - 64s - sparse_categorical_accuracy: 0.9899 - student_loss: 0.0631 - distillation_loss: 0.0102 - val_sparse_categorical_accuracy: 0.9881 - val_student_loss: 0.0277\n","Epoch 5/10\n","391/391 - 64s - sparse_categorical_accuracy: 0.9920 - student_loss: 0.0079 - distillation_loss: 0.0074 - val_sparse_categorical_accuracy: 0.9873 - val_student_loss: 0.0123\n","Epoch 6/10\n","391/391 - 64s - sparse_categorical_accuracy: 0.9936 - student_loss: 0.0028 - distillation_loss: 0.0069 - val_sparse_categorical_accuracy: 0.9891 - val_student_loss: 0.0092\n","Epoch 7/10\n","391/391 - 64s - sparse_categorical_accuracy: 0.9951 - student_loss: 0.0283 - distillation_loss: 0.0059 - val_sparse_categorical_accuracy: 0.9882 - val_student_loss: 0.0166\n","Epoch 8/10\n","391/391 - 64s - sparse_categorical_accuracy: 0.9968 - student_loss: 0.0068 - distillation_loss: 0.0050 - val_sparse_categorical_accuracy: 0.9891 - val_student_loss: 0.0037\n","Epoch 9/10\n","391/391 - 64s - sparse_categorical_accuracy: 0.9967 - student_loss: 0.0055 - distillation_loss: 0.0049 - val_sparse_categorical_accuracy: 0.9905 - val_student_loss: 0.0053\n","Epoch 10/10\n","391/391 - 64s - sparse_categorical_accuracy: 0.9969 - student_loss: 0.0099 - distillation_loss: 0.0043 - val_sparse_categorical_accuracy: 0.9894 - val_student_loss: 0.0114\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f65923c5e10>"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"IRbvSQWl_Ytl","outputId":"50959ef8-ad70-4927-cb62-9a0ad9d5b680"},"source":["latest_distiller = load_latest_checkpoint(f\"./ckpt/{distiller.name}\", is_distiller = True)\n","latest_distiller.evaluate(ts_ds, verbose = 2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Load latest checkpoints: ./ckpt/distiller/cp-008-0.0037.ckpt...\n","Restored model: distillation_model_wrapper_4\n","\n","79/79 - 5s - sparse_categorical_accuracy: 0.9928 - student_loss: 1.2945\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.9927999973297119, 1.2945350408554077]"]},"metadata":{"tags":[]},"execution_count":70}]},{"cell_type":"markdown","metadata":{"id":"o-l5NWlh_Ytm"},"source":["## **Commit to Tensorboard Dev.**"]},{"cell_type":"code","metadata":{"id":"TP2lzdLF_Ytm"},"source":["!tensorboard dev upload --logdir ./logs \\\n","    --name \"Experiment of 'Distilling the Knowledge in a Neural Network'\" \\\n","    --description \"Implemented training results from the paper 'https://arxiv.org/abs/1503.02531'\" \\\n","    --one_shot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"X7rzxK_t_Ytm","executionInfo":{"status":"ok","timestamp":1618884319753,"user_tz":-540,"elapsed":1049,"user":{"displayName":"Myung-Gyo Oh","photoUrl":"","userId":"01040732127983096879"}},"outputId":"493b2d79-36e2-406d-af03-66bd1646d48e"},"source":["from IPython import display\n","\n","display.IFrame(\n","    src = \"https://tensorboard.dev/experiment/up3gbYoJTNWgPkAmzYexNw/\",\n","    width = \"100%\",\n","    height = \"1000px\"\n",")"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <iframe\n","            width=\"100%\"\n","            height=\"1000px\"\n","            src=\"https://tensorboard.dev/experiment/up3gbYoJTNWgPkAmzYexNw/\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7f146fe35e10>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"FodujcJTnKxG"},"source":[""],"execution_count":null,"outputs":[]}]}