{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "From Here to There: Video Inbetweening Using Direct 3D Convolutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4hue1NUdIfz"
      },
      "source": [
        "## **From Here to There: Video Inbetweening Using Direct 3D Convolutions**\r\n",
        "\r\n",
        "Li, Y., Roblek, D., & Tagliasacchi, M. (2019). From here to there: Video inbetweening using direct 3d convolutions. arXiv preprint arXiv:1905.10240."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QfKF2fD7dIcd",
        "outputId": "5ac89d48-0bf7-467d-cc79-01a774e9702e"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YC2UkGEQ9IM",
        "outputId": "d5e3a72c-60a8-4d75-a390-c0457a04118c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb  8 01:31:32 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8R4-FbtT8Tz"
      },
      "source": [
        "## **Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0F4oboTBkGL"
      },
      "source": [
        "import datetime\r\n",
        "import os\r\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3uFnBj3T8QR"
      },
      "source": [
        "class HParams(object):\r\n",
        "    def __init__(self):\r\n",
        "        ## Noise vector.\r\n",
        "        self.D = 120\r\n",
        "\r\n",
        "        ## Original image/video.\r\n",
        "        self.T = 16\r\n",
        "        self.H_0 = 64\r\n",
        "        self.W_0 = 64\r\n",
        "        self.channels = 3\r\n",
        "\r\n",
        "        ## Feature map.\r\n",
        "        self.H = 8\r\n",
        "        self.W = 8\r\n",
        "        self.C = 64\r\n",
        "\r\n",
        "        self.L = 24\r\n",
        "        \r\n",
        "        ## Image/video/feature map size.\r\n",
        "        self.u_sz     = [self.D, ]\r\n",
        "        self.image_sz = [self.H_0, self.W_0, self.channels]\r\n",
        "        self.video_sz = [self.T] + self.image_sz\r\n",
        "\r\n",
        "        self.E_x_sz   = [self.H, self.W, self.C]\r\n",
        "        self.z_sz     = [self.T, self.H, self.W, self.C]\r\n",
        "\r\n",
        "        ## Train.\r\n",
        "        self.epochs = 5\r\n",
        "        self.batch_sz = 32\r\n",
        "        \r\n",
        "HPARAMS = HParams()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSC377yoh-Q1"
      },
      "source": [
        "## **Model Architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGKxQcdDkJCb"
      },
      "source": [
        "### **Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVAvXk1dIZR"
      },
      "source": [
        "def Conv2D_BN_LeakyReLU(\r\n",
        "    x, \r\n",
        "    filters, \r\n",
        "    kernel_sz, \r\n",
        "    strides = 1, \r\n",
        "    padding = \"same\",\r\n",
        "):\r\n",
        "    x = tf.keras.layers.Conv2D(filters, kernel_sz, strides = strides, padding = padding)(x)\r\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\r\n",
        "    x = tf.keras.layers.Activation(tf.nn.leaky_relu)(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def ImageEncoder(\r\n",
        "    model_name = \"ImageEncoder\",\r\n",
        "):\r\n",
        "    model_input = tf.keras.layers.Input(shape = HPARAMS.image_sz, dtype = tf.dtypes.float32)\r\n",
        "    \r\n",
        "    args = [\r\n",
        "        [ 64, 4, 2],\r\n",
        "        [ 64, 3, 1],\r\n",
        "        [128, 4, 2],\r\n",
        "        [128, 3, 1],\r\n",
        "        [256, 4, 2],\r\n",
        "        [256, 3, 1],\r\n",
        "        [ 64, 3, 1]]\r\n",
        "\r\n",
        "    x = model_input\r\n",
        "\r\n",
        "    ## L1 to L7.\r\n",
        "    for (filters, kernel_sz, strides) in args:\r\n",
        "        x = Conv2D_BN_LeakyReLU(x, filters, kernel_sz, strides = strides)\r\n",
        "\r\n",
        "    model_output = x\r\n",
        "\r\n",
        "    return tf.keras.Model(\r\n",
        "        inputs = model_input,\r\n",
        "        outputs = model_output,\r\n",
        "        name = model_name)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KilcGxzMaJGg"
      },
      "source": [
        "# tmp = ImageEncoder()\r\n",
        "# tmp.summary()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgHSq5rcaOVO"
      },
      "source": [
        "# tf.keras.utils.plot_model(tmp, show_shapes = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR0dqxGIaOSe"
      },
      "source": [
        "# del tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI2fWvt0aHi9"
      },
      "source": [
        "def Conv3D_T_BN_LeakyReLU(\r\n",
        "    x, \r\n",
        "    filters, \r\n",
        "    kernel_sz, \r\n",
        "    strides = 1, \r\n",
        "    padding = \"same\"\r\n",
        "):\r\n",
        "    x = tf.keras.layers.Conv3DTranspose(filters, kernel_sz, strides = strides, padding = padding)(x)\r\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\r\n",
        "    x = tf.keras.layers.Activation(tf.nn.leaky_relu)(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def VideoGenerator(\r\n",
        "    model_name = \"VideoGenerator\",\r\n",
        "):\r\n",
        "    model_input = tf.keras.layers.Input(shape = HPARAMS.z_sz, dtype = tf.dtypes.float32)\r\n",
        "\r\n",
        "    args = [\r\n",
        "        [256, (3, 3, 3), (1, 1, 1)],\r\n",
        "        [256, (3, 3, 3), (1, 1, 1)],\r\n",
        "        [128, (3, 4, 4), (1, 2, 2)],\r\n",
        "        [128, (3, 3, 3), (1, 1, 1)],\r\n",
        "        [ 64, (3, 4, 4), (1, 2, 2)],\r\n",
        "        [ 64, (3, 3, 4), (1, 1, 1)],\r\n",
        "        [  3, (3, 4, 4), (1, 2, 2)]]\r\n",
        "    \r\n",
        "    x = model_input\r\n",
        "\r\n",
        "    ## L1 to L7.\r\n",
        "    for (filters, kernel_sz, strides) in args:\r\n",
        "        x = Conv3D_T_BN_LeakyReLU(x, filters, kernel_sz, strides = strides)\r\n",
        "\r\n",
        "    model_output = x\r\n",
        "\r\n",
        "    return tf.keras.Model(\r\n",
        "        inputs = model_input,\r\n",
        "        outputs = model_output,\r\n",
        "        name = model_name)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MHyB9GFa0Ec"
      },
      "source": [
        "# tmp = VideoGenerator()\r\n",
        "# tmp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOpXbbHsa1Ze"
      },
      "source": [
        "# tf.keras.utils.plot_model(tmp, show_shapes = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12MSvbwfa0Bx"
      },
      "source": [
        "# del tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9phsKnv2Sxv"
      },
      "source": [
        "class LatentRepresentationGeneratorBlock(tf.keras.Model):\r\n",
        "    def __init__(\r\n",
        "        self, \r\n",
        "        l, \r\n",
        "        T_l, \r\n",
        "        H = HPARAMS.H, \r\n",
        "        W = HPARAMS.W, \r\n",
        "        C = HPARAMS.C,\r\n",
        "        model_name = \"LatentRepresentationGeneratorBlock\"\r\n",
        "    ):\r\n",
        "        super(LatentRepresentationGeneratorBlock, self).__init__(name = f\"{model_name}_{l}\")\r\n",
        "\r\n",
        "        self.T_l = T_l\r\n",
        "        self.H = H\r\n",
        "        self.W = W\r\n",
        "        self.C = C\r\n",
        "\r\n",
        "        self.linear = tf.keras.layers.Dense(self.T_l * self.C)\r\n",
        "\r\n",
        "        self.conv1d_s = tf.keras.layers.Conv1D(self.C, 3, strides = 1, padding = \"same\")\r\n",
        "        self.conv1d_e = tf.keras.layers.Conv1D(self.C, 3, strides = 1, padding = \"same\")\r\n",
        "        self.conv1d_n = tf.keras.layers.Conv1D(self.C, 3, strides = 1, padding = \"same\")\r\n",
        "\r\n",
        "        self.conv3d_1 = tf.keras.layers.Conv3D(self.C, 3, strides = 1, padding = \"same\")\r\n",
        "        self.conv3d_2 = tf.keras.layers.Conv3D(self.C, 3, strides = 1, padding = \"same\")\r\n",
        "\r\n",
        "\r\n",
        "    def call(\r\n",
        "        self, \r\n",
        "        u, \r\n",
        "        E_xs, \r\n",
        "        E_xe, \r\n",
        "        z_last\r\n",
        "    ):        \r\n",
        "        u_l = self.linear(u)\r\n",
        "        u_l = tf.reshape(u_l, (-1, self.T_l, self.C)) ## [batch, T_l, C]\r\n",
        "\r\n",
        "        g_e = tf.nn.sigmoid(self.conv1d_e(u_l))\r\n",
        "        g_e = tf.tile(g_e[:, :, tf.newaxis, tf.newaxis, :], [1, 1, self.H, self.W, 1])\r\n",
        "\r\n",
        "        g_s = tf.nn.sigmoid(self.conv1d_s(u_l))\r\n",
        "        g_s = tf.tile(g_s[:, :, tf.newaxis, tf.newaxis, :], [1, 1, self.H, self.W, 1])\r\n",
        "        \r\n",
        "        x = tf.math.maximum(0., 1 - g_s - g_e)\r\n",
        "\r\n",
        "        n = self.conv1d_n(u_l)\r\n",
        "        n = tf.tile(n[:, :, tf.newaxis, tf.newaxis, :], [1, 1, self.H, self.W, 1])\r\n",
        "\r\n",
        "        E_xs = tf.tile(E_xs[:, tf.newaxis, ...], [1, self.T_l, 1, 1, 1])\r\n",
        "        E_xe = tf.tile(E_xe[:, tf.newaxis, ...], [1, self.T_l, 1, 1, 1])\r\n",
        "\r\n",
        "        # print(f\"g_s.shape: {g_s.shape}\")\r\n",
        "        # print(f\"E_xs.shape: {E_xs.shape}\")\r\n",
        "\r\n",
        "        z = g_s * E_xs + g_e * E_xe + x * z_last + n\r\n",
        "        z_residual = z\r\n",
        "        \r\n",
        "        z = tf.nn.leaky_relu(self.conv3d_1(z))\r\n",
        "        z = tf.nn.leaky_relu(self.conv3d_2(z) + z_residual)\r\n",
        "\r\n",
        "        return z\r\n",
        "        \r\n",
        "\r\n",
        "def LatentRepresentationGenerator(\r\n",
        "    image_encoder,\r\n",
        "    video_generator,\r\n",
        "    T = HPARAMS.T,\r\n",
        "    L = HPARAMS.L,\r\n",
        "    model_name = \"LatentRepresentationGenerator\",\r\n",
        "):\r\n",
        "    model_input_1 = tf.keras.layers.Input(shape = HPARAMS.u_sz, dtype = tf.dtypes.float32) ## u\r\n",
        "    model_input_2 = tf.keras.layers.Input(shape = HPARAMS.image_sz, dtype = tf.dtypes.float32) ## E(x_s)\r\n",
        "    model_input_3 = tf.keras.layers.Input(shape = HPARAMS.image_sz, dtype = tf.dtypes.float32) ## E(x_e)\r\n",
        "\r\n",
        "    u = model_input_1\r\n",
        "    E_xs = image_encoder(model_input_2)\r\n",
        "    E_xe = image_encoder(model_input_3)\r\n",
        "\r\n",
        "    z_last = tf.keras.layers.Lambda(lambda xs: tf.stack(xs, axis = 1))([E_xs, E_xe])\r\n",
        "    \r\n",
        "    for l in range(L):\r\n",
        "        if not (l % 8):\r\n",
        "            z_last = tf.keras.layers.UpSampling3D((2, 1, 1))(z_last)\r\n",
        "\r\n",
        "        T_l = int(T / 2 ** (2 - l // 8))\r\n",
        "        z_last = LatentRepresentationGeneratorBlock(l, T_l)(u, E_xs, E_xe, z_last)\r\n",
        "\r\n",
        "    model_output = video_generator(z_last)\r\n",
        "\r\n",
        "    return tf.keras.Model(\r\n",
        "        inputs = [model_input_1, model_input_2, model_input_3],\r\n",
        "        outputs = model_output,\r\n",
        "        name = model_name)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOU2MrC8bCGs"
      },
      "source": [
        "# foo = ImageEncoder()\r\n",
        "# bar = VideoGenerator()\r\n",
        "\r\n",
        "# tmp = LatentRepresentationGenerator(foo, bar)\r\n",
        "# tmp.summary()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEb7bcwRbDTt"
      },
      "source": [
        "# tf.keras.utils.plot_model(tmp, show_shapes = True, rankdir = \"LR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNXy4ptWdvUx"
      },
      "source": [
        "# del tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ywY6dsxdvRZ"
      },
      "source": [
        "### **Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-2H0UdadvOG"
      },
      "source": [
        "def Conv3D_LN_LeakyReLU(\r\n",
        "    x, \r\n",
        "    filters, \r\n",
        "    kernel_sz, \r\n",
        "    strides = 1, \r\n",
        "    padding = \"same\"\r\n",
        "):\r\n",
        "    x = tf.keras.layers.Conv3D(filters, kernel_sz, strides = strides, padding = padding)(x)\r\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\r\n",
        "    x = tf.keras.layers.Activation(tf.nn.leaky_relu)(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def VideoDiscriminator(\r\n",
        "    model_name = \"VideoDiscriminator\",\r\n",
        "):\r\n",
        "    \"\"\"MoCoGAN-style\"\"\"\r\n",
        "    model_input = tf.keras.layers.Input(shape = HPARAMS.video_sz, dtype = tf.dtypes.float32)\r\n",
        "    x = model_input\r\n",
        "\r\n",
        "    args = [\r\n",
        "        [ 64, 4, (1, 2, 2)],\r\n",
        "        [128, 4, (1, 2, 2)],\r\n",
        "        [256, 4, (1, 2, 2)],\r\n",
        "        [512, 4, (1, 2, 2)]]\r\n",
        "    \r\n",
        "    ## L1 to L4.\r\n",
        "    for (filters, kernel_sz, strides) in args:\r\n",
        "        x = tf.keras.layers.ZeroPadding3D(padding = (0, 1, 1))(x)\r\n",
        "        x = Conv3D_LN_LeakyReLU(x, filters, kernel_sz, strides = strides, padding = \"valid\")\r\n",
        "\r\n",
        "    ## L5.\r\n",
        "    x = tf.keras.layers.Flatten()(x)\r\n",
        "    x = tf.keras.layers.Dense(1)(x)\r\n",
        "    model_output = tf.keras.layers.Activation(tf.nn.sigmoid)(x)\r\n",
        "\r\n",
        "    return tf.keras.Model(\r\n",
        "        inputs = model_input,\r\n",
        "        outputs = model_output,\r\n",
        "        name = model_name)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO7sEoLHgSIs"
      },
      "source": [
        "# tmp = VideoDiscriminator()\r\n",
        "# tmp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1nLP9w1giwd"
      },
      "source": [
        "# tf.keras.utils.plot_model(tmp, show_shapes = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0DX5Q9HgmEt"
      },
      "source": [
        "# del tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JxBUjnIgQKQ"
      },
      "source": [
        "def Conv2D_LN_LeakyReLU(\r\n",
        "    x, \r\n",
        "    filters, \r\n",
        "    kernel_sz, \r\n",
        "    strides = 1, \r\n",
        "    padding = \"same\"\r\n",
        "):\r\n",
        "    x = tf.keras.layers.Conv2D(filters, kernel_sz, strides = strides, padding = padding)(x)\r\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\r\n",
        "    x = tf.keras.layers.Activation(tf.nn.leaky_relu)(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def Shortcut(\r\n",
        "    x, \r\n",
        "    filters,\r\n",
        "    kernel_sz = 1,\r\n",
        "    pool_kernel_sz = 2,\r\n",
        "    pool_strides = 2,\r\n",
        "    pool_padding = \"same\"\r\n",
        "):\r\n",
        "    x = tf.keras.layers.AveragePooling2D(pool_kernel_sz, strides = pool_strides, padding = pool_padding)(x)\r\n",
        "    x = tf.keras.layers.Conv2D(filters, kernel_sz)(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "\r\n",
        "def ImageDiscriminator(\r\n",
        "    model_name = \"ImageDiscriminator\",\r\n",
        "):\r\n",
        "    \"\"\"Resnet-based\"\"\"\r\n",
        "    model_input = tf.keras.layers.Input(shape = HPARAMS.image_sz, dtype = tf.dtypes.float32)\r\n",
        "\r\n",
        "    ## L1.\r\n",
        "    x = tf.keras.layers.Conv2D(3, 3, padding = \"same\")(model_input)\r\n",
        "\r\n",
        "    ## L2 to L8.\r\n",
        "    for filters in [64, 128, 256, 512]:\r\n",
        "        residual = Shortcut(x, filters = filters)\r\n",
        "        x = Conv2D_LN_LeakyReLU(x, filters, 4, strides = 2)\r\n",
        "        x = Conv2D_LN_LeakyReLU(x, filters, 3, strides = 1)\r\n",
        "        x = tf.keras.layers.Add()([x, residual])\r\n",
        "\r\n",
        "    ## L9.\r\n",
        "    x = tf.keras.layers.Flatten()(x)\r\n",
        "    x = tf.keras.layers.Dense(1)(x)\r\n",
        "    model_output = tf.keras.layers.Activation(tf.nn.sigmoid)(x)\r\n",
        "\r\n",
        "    return tf.keras.Model(\r\n",
        "        inputs = model_input,\r\n",
        "        outputs = model_output,\r\n",
        "        name = model_name)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x8mkjXAbDP2"
      },
      "source": [
        "# tmp = ImageDiscriminator()\r\n",
        "# tmp.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjEwZS1th3pn"
      },
      "source": [
        "# tf.keras.utils.plot_model(tmp, show_shapes = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHaUf5uObDLs"
      },
      "source": [
        "# del tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dRjIaEtiE9m"
      },
      "source": [
        "## **Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZV4WOWkq5-1"
      },
      "source": [
        "def DiscriminatorLoss(\r\n",
        "    real_output,\r\n",
        "    generated_output,\r\n",
        "    epsilon = 1e-7\r\n",
        "):\r\n",
        "    \"\"\"Adopting the non-saturating log-loss for discriminators.\"\"\"\r\n",
        "    real_loss = -1 * tf.math.log(real_output + epsilon)\r\n",
        "    generated_loss = tf.math.log(tf.ones_like(generated_output) - generated_output + epsilon)\r\n",
        "    total_disc_loss = tf.math.reduce_mean(real_loss + generated_loss)\r\n",
        "\r\n",
        "    return total_disc_loss\r\n",
        "\r\n",
        "\r\n",
        "def GeneratorLoss(\r\n",
        "    generated_video_output,\r\n",
        "    generated_image_output,\r\n",
        "    epsilon = 1e-7,\r\n",
        "):\r\n",
        "    \"\"\"Loss function for encoder, feature map generator, and video generator.\"\"\"\r\n",
        "    generated_video_loss = -1 * tf.math.reduce_mean(tf.math.log(generated_video_output + epsilon))\r\n",
        "    generated_image_loss = -1 * tf.math.reduce_mean(tf.math.log(generated_image_output + epsilon))\r\n",
        "    total_gen_loss = generated_video_loss + generated_image_loss\r\n",
        "\r\n",
        "    return total_gen_loss"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlwY604c8_pq",
        "outputId": "377e8581-ac0a-4061-a71a-bafec743b35e"
      },
      "source": [
        "## D_V\r\n",
        "foo = tf.ones((32, 16, 64, 64, 3))  ## real_output\r\n",
        "bar = tf.zeros((32, 16, 64, 64, 3)) ## generated_output\r\n",
        "\r\n",
        "DiscriminatorLoss(foo, bar)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE186K-v8_nI",
        "outputId": "f5310812-259c-46dc-ef2f-66acd3141949"
      },
      "source": [
        "## D_I\r\n",
        "foo = tf.ones((32, 14, 64, 64, 3))  ## real_output\r\n",
        "bar = tf.zeros((32, 14, 64, 64, 3)) ## generated_output\r\n",
        "\r\n",
        "DiscriminatorLoss(foo, bar)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnhB4xvvisUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd8e4ff-8636-498b-e14d-470e0fc20120"
      },
      "source": [
        "## G := {E, G_Z, G_V}\r\n",
        "foo = tf.ones((32, 16, 64, 64, 3))          ## generated_video_output\r\n",
        "bar = tf.ones((32, 14, 64, 64, 3)) ## generated_image_output\r\n",
        "\r\n",
        "GeneratorLoss(foo, bar)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=-2.3841855e-07>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXfLx5opisOc"
      },
      "source": [
        "## **Fit**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIZlakTFB9FG"
      },
      "source": [
        "### **Generate Each Parts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt_p4btjB8h-"
      },
      "source": [
        "## Generator.\r\n",
        "latent_representation_generator = LatentRepresentationGenerator(\r\n",
        "    image_encoder = ImageEncoder(),\r\n",
        "    video_generator = VideoGenerator())\r\n",
        "\r\n",
        "## Discriminator.\r\n",
        "video_discriminator = VideoDiscriminator()\r\n",
        "image_discriminator = ImageDiscriminator()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLRdrQpJCB-R"
      },
      "source": [
        "### **Optimizers and Checkpoints**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m65S4bjv_ZTN"
      },
      "source": [
        "generator_optimizer           = tf.keras.optimizers.Adam(lr = 5e-5, beta_1 = 0.5, beta_2 = 0.999, epsilon = 1e-8)\r\n",
        "video_discriminator_optimizer = tf.keras.optimizers.Adam(lr = 5e-5, beta_1 = 0.5, beta_2 = 0.999, epsilon = 1e-8)\r\n",
        "image_discriminator_optimizer = tf.keras.optimizers.Adam(lr = 5e-5, beta_1 = 0.5, beta_2 = 0.999, epsilon = 1e-8)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEJ5s5Ou_ZQi"
      },
      "source": [
        "checkpoint_dir = \"./training_checkpoints\"\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\n",
        "checkpoint = tf.train.Checkpoint(\r\n",
        "    ## Optimizers.\r\n",
        "    generator_optimizer = generator_optimizer,\r\n",
        "    video_discriminator_optimizer = video_discriminator_optimizer,\r\n",
        "    image_discriminator_optimizer = image_discriminator_optimizer,\r\n",
        "\r\n",
        "    ## Generators.\r\n",
        "    latent_representation_generator = latent_representation_generator,\r\n",
        "    \r\n",
        "    ## Discriminators.\r\n",
        "    video_discriminator = video_discriminator,\r\n",
        "    image_discriminator = image_discriminator)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0eu3H0z_ZNj"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogS44G2nYlNL"
      },
      "source": [
        "# !rm -rf logs"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LM8jMcD_ZK4"
      },
      "source": [
        "log_dir = \"logs/\"\r\n",
        "\r\n",
        "summary_writer = tf.summary.create_file_writer(\r\n",
        "    log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmwUwQBBiE2q"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(videos, epoch):\r\n",
        "    noise = tf.random.normal([videos.shape[0], HPARAMS.D]) ## not [HPARAMS.batch_sz, HPARAMS.D]\r\n",
        "\r\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as video_disc_tape, tf.GradientTape() as image_disc_tape:\r\n",
        "        ## Split key frames.\r\n",
        "        x_s = videos[:, 0]\r\n",
        "        x_e = videos[:, -1]\r\n",
        "\r\n",
        "        ## Genearte videos.\r\n",
        "        generated_videos = latent_representation_generator([noise, x_s, x_e], training = True)\r\n",
        "\r\n",
        "        ## Discriminate videos/images.\r\n",
        "        video_disc_output_for_real = video_discriminator(videos, training = True)\r\n",
        "        video_disc_output_for_gen  = video_discriminator(generated_videos, training = True)\r\n",
        "\r\n",
        "        image_disc_output_for_real = tf.stack([\r\n",
        "            image_discriminator(image, training = True) \\\r\n",
        "            for image in tf.unstack(videos, axis = 1)[1:-1]], axis = 1)\r\n",
        "        image_disc_output_for_gen  = tf.stack([\r\n",
        "            image_discriminator(generated_image, training = True) \\\r\n",
        "            for generated_image in tf.unstack(generated_videos, axis = 1)[1:-1]], axis = 1)\r\n",
        "        \r\n",
        "        ## Calculate losses.\r\n",
        "        gen_loss = GeneratorLoss(video_disc_output_for_gen, image_disc_output_for_gen) \r\n",
        "        disc_video_loss = DiscriminatorLoss(video_disc_output_for_real, video_disc_output_for_gen)\r\n",
        "        disc_image_loss = DiscriminatorLoss(image_disc_output_for_real[1:-1], image_disc_output_for_gen[1:-1])\r\n",
        "\r\n",
        "    ## Calculate and apply gradients.\r\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, latent_representation_generator.trainable_variables)\r\n",
        "    gradients_of_video_discriminator = video_disc_tape.gradient(disc_video_loss, video_discriminator.trainable_variables)\r\n",
        "    gradients_of_image_discriminator = image_disc_tape.gradient(disc_image_loss, image_discriminator.trainable_variables)\r\n",
        "        \r\n",
        "    generator_optimizer.apply_gradients(\r\n",
        "        zip(gradients_of_generator, latent_representation_generator.trainable_variables))\r\n",
        "    video_discriminator_optimizer.apply_gradients(\r\n",
        "        zip(gradients_of_video_discriminator, video_discriminator.trainable_variables))\r\n",
        "    image_discriminator_optimizer.apply_gradients(\r\n",
        "        zip(gradients_of_image_discriminator, image_discriminator.trainable_variables))\r\n",
        "    \r\n",
        "    ## Record loss graph.\r\n",
        "    with summary_writer.as_default():\r\n",
        "        tf.summary.scalar(\"gen_loss\", gen_loss, step = epoch)\r\n",
        "        tf.summary.scalar(\"disc_video_loss\", disc_video_loss, step = epoch)\r\n",
        "        tf.summary.scalar(\"disc_image_loss\", disc_image_loss, step = epoch)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuAq6EkmKqzp"
      },
      "source": [
        "def train(\r\n",
        "    dataset, \r\n",
        "    epochs = HPARAMS.epochs,\r\n",
        "):\r\n",
        "    for epoch in range(epochs):\r\n",
        "        start = time.time()\r\n",
        "\r\n",
        "        for image_batch in dataset:\r\n",
        "            train_step(image_batch, epoch)\r\n",
        "\r\n",
        "        ## Save model every epochs.\r\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\r\n",
        "\r\n",
        "        ## Display training times of each epochs.\r\n",
        "        print (f\"Time for epoch {epoch + 1} is {time.time() - start:.2f} sec\")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IesDatxFLgrJ",
        "outputId": "6f93e353-f9ad-44da-f5ef-70c83e17c4a8"
      },
      "source": [
        "%%time\r\n",
        "## Dummy training dataset with 100 videos.\r\n",
        "dummy_tr_tensor = tf.random.uniform(shape = [100] + HPARAMS.video_sz, maxval = 1.)\r\n",
        "dummy_tr_dataset = tf.data.Dataset.from_tensor_slices(dummy_tr_tensor) \\\r\n",
        "                            .batch(HPARAMS.batch_sz) \\\r\n",
        "                            .cache() \\\r\n",
        "                            .prefetch(-1)\r\n",
        "\r\n",
        "train(dummy_tr_dataset)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 103.76 sec\n",
            "Time for epoch 2 is 80.82 sec\n",
            "WARNING:tensorflow:5 out of the last 9 calls to <function train_step at 0x7fe74d03ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fe74d03ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Time for epoch 3 is 80.79 sec\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function train_step at 0x7fe74d03ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fe74d03ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Time for epoch 4 is 80.33 sec\n",
            "WARNING:tensorflow:7 out of the last 13 calls to <function train_step at 0x7fe74d03ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function train_step at 0x7fe74d03ec80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Time for epoch 5 is 80.09 sec\n",
            "CPU times: user 6min 54s, sys: 13.1 s, total: 7min 7s\n",
            "Wall time: 7min 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbd1NPrwV-Wi"
      },
      "source": [
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH3KzHGbc-Fe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}