{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Communication-Efficient Learning of Deep Networks from Decentralized Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA_dbRke8sDc"
      },
      "source": [
        "# **Communication-Efficient Learning of Deep Networks from Decentralized Data**\r\n",
        "\r\n",
        "McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017, April). Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics (pp. 1273-1282). PMLR.\r\n",
        "\r\n",
        "Ref: *https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0knEqU-s0pKd"
      },
      "source": [
        "The implementation environment is as follows.\r\n",
        "\r\n",
        "* ML Framework\r\n",
        "\r\n",
        "    * Python 3.6.9\r\n",
        "    * Tensorflow 2.4.1\r\n",
        "    * Tensorflow-Federated 0.18.0\r\n",
        "\r\n",
        "* Hardware\r\n",
        "\r\n",
        "    * GPU: NVIDIA Tesla V100 (16G)\r\n",
        "    * RAM: 25G\r\n",
        "\r\n",
        "* Dataset & Model\r\n",
        "\r\n",
        "    * MNIST (#: 60,000)\r\n",
        "    * Base 2D-CNN (introduced in the paper)\r\n",
        "\r\n",
        "* Parameter\r\n",
        "\r\n",
        "    * C = 0.1\r\n",
        "    * B = 10\r\n",
        "    * E = 1\r\n",
        "\r\n",
        "where C, B, E mean as:\r\n",
        "\r\n",
        "* C: the fraction of clients that performs on each round\r\n",
        "    \r\n",
        "    (한 번의 가중치 업데이트에 참여하는 클라이언트 비율)\r\n",
        "\r\n",
        "* E: the number of training passes each client makes over its local dataset on each round\r\n",
        "\r\n",
        "    (하나의 클라이언트가 한 번의 라운드에서 수행하는 연산(배치 iter)의 횟수)\r\n",
        "\r\n",
        "* B: the local minibatch size used for the client updates \r\n",
        "\r\n",
        "    (하나의 클라이언트에 할당되는 미니배치 크기)\r\n",
        "\r\n",
        "Therefore, the content executed in this code will show a result similar to the red solid line in the upper left and upper right graphs in Figure 2 of the paper.\r\n",
        "\r\n",
        "The results are commited on Tensorboard.dev.\r\n",
        "\r\n",
        "*https://tensorboard.dev/experiment/BmDncAGATNmukm7VdNxR9w/*\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4x1y4BtHVcs"
      },
      "source": [
        "In order to present convincing experimental results, the client's choice is important. Depending on whether `tf.data.Dataset.shard()` or `tf.data.Dataset.take()` is used, and depending on the selection of `client_dataset_list[i::num_clients_per_round]` or `client_dataset_list[i*num_clients_per_round:(i+1)*num_clients_per_round]`, the experimental results will be greatly biased. Is expected. However, we did not proceed with this, as it is far beyond the scope of this code, and it will be left as a future research project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75aJ4k_BSlH5"
      },
      "source": [
        "## **Default Setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMn5SkPiSlE4"
      },
      "source": [
        "!pip install --quiet --upgrade tensorflow_federated\r\n",
        "!pip install --quiet --upgrade nest_asyncio\r\n",
        "\r\n",
        "import nest_asyncio\r\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFt31DKVSlCM",
        "outputId": "805144e3-6bcb-482e-dd10-153ad2d6c7a8"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_federated as tff\r\n",
        "\r\n",
        "print(f\" tf.__version__: {tf.__version__}\")\r\n",
        "print(f\"tff.__version__: {tff.__version__}\")\r\n",
        "\r\n",
        "import collections\r\n",
        "import sklearn\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "np.random.seed(0)\r\n",
        "\r\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " tf.__version__: 2.4.1\n",
            "tff.__version__: 0.18.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn356VBh5ZiU",
        "outputId": "1b8d3a77-5328-4fae-fe72-7bc870979198"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 10 06:20:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    33W / 250W |   8767MiB / 16280MiB |      3%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUznS-I7Sk_n"
      },
      "source": [
        "## **Load MNIST Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6_F8cNcS4wJ",
        "outputId": "8faf20cf-34a2-400b-d4b9-59a06082d7fb"
      },
      "source": [
        "## Load MNIST datasets.\r\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\r\n",
        "\r\n",
        "## Print the shapes.\r\n",
        "print(f\"train_images.shape: {train_images.shape}\")\r\n",
        "print(f\"train_labels.shape: {train_labels.shape}\")\r\n",
        "print(f\"test_images.shape: {test_images.shape}\")\r\n",
        "print(f\"test_labels.shape: {test_labels.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "train_images.shape: (60000, 28, 28)\n",
            "train_labels.shape: (60000,)\n",
            "test_images.shape: (10000, 28, 28)\n",
            "test_labels.shape: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Lymr2GpaUx"
      },
      "source": [
        "SHUFFLE_BUFFER = 100_000\r\n",
        "BATCH_SIZE = 10\r\n",
        "PREFETCH_BUFFER = 100\r\n",
        "\r\n",
        "def preprocess_mnist_cnn(dataset, iid):\r\n",
        "    assert iid in [True, False]\r\n",
        "\r\n",
        "    def batch_rescaling_fn(pixels, label):\r\n",
        "        return (\r\n",
        "            tf.image.convert_image_dtype(pixels, tf.dtypes.float32), \r\n",
        "            tf.cast(label, tf.int32))\r\n",
        "\r\n",
        "    def batch_format_fn(pixels, label):\r\n",
        "        \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\r\n",
        "        return collections.OrderedDict(\r\n",
        "            x = tf.reshape(pixels, [-1, 28, 28, 1]),\r\n",
        "            y = tf.reshape(label, [-1, 1]))\r\n",
        "\r\n",
        "    if iid:\r\n",
        "        dataset = dataset.shuffle(SHUFFLE_BUFFER)\r\n",
        "\r\n",
        "    return dataset.batch(BATCH_SIZE).map(batch_rescaling_fn).map(\r\n",
        "        batch_format_fn).prefetch(PREFETCH_BUFFER)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrpU-LiflmlY"
      },
      "source": [
        "### **IID MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k01MfR0gUiA8",
        "outputId": "904a65fe-9d61-4010-97de-07936acfbd0b"
      },
      "source": [
        "## IID, where the data is shuffled, and then partitioned \r\n",
        "## into 100 clients each receiving 600 examples.\r\n",
        "iid_tr_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\r\n",
        "iid_tr_ds = preprocess_mnist_cnn(iid_tr_ds, iid = True)\r\n",
        "\r\n",
        "## Shard it.\r\n",
        "num_clients = 100\r\n",
        "federated_iid_tr_ds = [iid_tr_ds.shard(num_clients, i) for i in range(num_clients)]\r\n",
        "\r\n",
        "print(f\"Number of client datasets: {len(federated_iid_tr_ds)}\")\r\n",
        "print(f\"First dataset: {federated_iid_tr_ds[0]}\")"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of client datasets: 100\n",
            "First dataset: <ShardDataset shapes: OrderedDict([(x, (None, 28, 28, 1)), (y, (None, 1))]), types: OrderedDict([(x, tf.float32), (y, tf.int32)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrcKg4_CaVTq",
        "outputId": "b4d19146-1654-4f32-9bab-71b6482e1dce"
      },
      "source": [
        "## Each clients have 60 mini-batches (each mini-batches have 10 items).\r\n",
        "len(list(federated_iid_tr_ds[0].as_numpy_iterator()))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ubPHyYclrqM"
      },
      "source": [
        "### **Non-IID MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_V7mGxOvDTX",
        "outputId": "86b22cca-1672-4eef-8c4c-091f9001a927"
      },
      "source": [
        "np.reshape(np.arange(200), (100, 2))[0]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHFbXbZNlrtR",
        "outputId": "fa3b7a09-f1b9-4322-d082-0129a20848d7"
      },
      "source": [
        "## Non-IID, where we first sort the data by digital label, \r\n",
        "## divide it into 200 shards of size 300, and assign each of 100 clients 2 shards.\r\n",
        "idx = np.argsort(train_labels)\r\n",
        "\r\n",
        "non_iid_tr_ds = tf.data.Dataset.from_tensor_slices((train_images[idx], train_labels[idx]))\r\n",
        "non_iid_tr_ds = preprocess_mnist_cnn(non_iid_tr_ds, iid = False)\r\n",
        "\r\n",
        "## Shard it.\r\n",
        "num_shards = 200 \r\n",
        "num_shard_size = 300 // BATCH_SIZE ## 15\r\n",
        "\r\n",
        "splits = [non_iid_tr_ds.skip(num_shard_size * i).take(num_shard_size) for i in range(num_shards)]\r\n",
        "federated_non_iid_tr_ds = [splits[i].concatenate(splits[i + 100]) for i in range(num_clients)]\r\n",
        "\r\n",
        "## Do we need to reshuffle the data set?\r\n",
        "# federated_non_iid_tr_ds = [ds.unbatch().shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE) for ds in federated_non_iid_tr_ds]\r\n",
        "\r\n",
        "print(f\"Number of client datasets: {len(federated_non_iid_tr_ds)}\")\r\n",
        "print(f\"First dataset: {federated_non_iid_tr_ds[0]}\")"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of client datasets: 100\n",
            "First dataset: <ConcatenateDataset shapes: OrderedDict([(x, (None, 28, 28, 1)), (y, (None, 1))]), types: OrderedDict([(x, tf.float32), (y, tf.int32)])>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYg6f4k1lrwS",
        "outputId": "37acb506-e2ed-4fb9-a5bf-0f25da8e83cc"
      },
      "source": [
        "for element in federated_non_iid_tr_ds[0].as_numpy_iterator():\r\n",
        "    print(np.reshape(element[\"y\"], (-1,)))"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 0 0 0]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n",
            "[4 4 4 4 4 4 4 4 4 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdRiMUoflr2D",
        "outputId": "e7421996-77e6-4675-aff4-e81db5ea4170"
      },
      "source": [
        "## Each clients have 60 mini-batches (each mini-batches have 10 items).\r\n",
        "len(list(federated_non_iid_tr_ds[0].as_numpy_iterator()))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvNRzTJDeNrd"
      },
      "source": [
        "## **Build Model based on tf.keras**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m8JtFjJgc_H"
      },
      "source": [
        "def create_keras_model():\r\n",
        "    \"\"\"Make MNIST-CNN model with 1,663,370 params.\"\"\"\r\n",
        "    return tf.keras.models.Sequential([\r\n",
        "        tf.keras.layers.Input(shape = [28, 28, 1], dtype = tf.dtypes.float32),\r\n",
        "        tf.keras.layers.Conv2D(32, 5, padding = \"same\", activation = \"relu\"),\r\n",
        "        tf.keras.layers.MaxPool2D(2, padding = \"same\"),\r\n",
        "        tf.keras.layers.Conv2D(64, 5, padding = \"same\", activation = \"relu\"),\r\n",
        "        tf.keras.layers.MaxPool2D(2, padding = \"same\"),\r\n",
        "        tf.keras.layers.Flatten(),\r\n",
        "        tf.keras.layers.Dense(512, activation = \"relu\"),\r\n",
        "        tf.keras.layers.Dense(10),\r\n",
        "        tf.keras.layers.Activation(tf.nn.softmax)], name = \"MNIST_CNN\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-XmQ3FHiVby",
        "outputId": "5cbf54ff-5209-417b-d275-4c29c1549e33"
      },
      "source": [
        "tmp = create_keras_model()\r\n",
        "tmp.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"MNIST_CNN\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 64)        51264     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               1606144   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,663,370\n",
            "Trainable params: 1,663,370\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn9oEwqFiV_8"
      },
      "source": [
        "del tmp"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5a4DZAqiSD6"
      },
      "source": [
        "def model_fn():\r\n",
        "    # We must create a new model here, and not capture it from an external\r\n",
        "    # scope. TFF will call this within different graph contexts.\r\n",
        "    keras_model = create_keras_model()\r\n",
        "    return tff.learning.from_keras_model(\r\n",
        "        keras_model,\r\n",
        "        input_spec = federated_iid_tr_ds[0].element_spec,\r\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy(),\r\n",
        "        metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRvOeY7FhRPJ"
      },
      "source": [
        "## **Training the Model on Federated Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA1m6U_6hRLm"
      },
      "source": [
        "## The client_optimizer is only used to compute local model updates on each client. \r\n",
        "## The server_optimizer applies the averaged update to the global model at the server.\r\n",
        "tmp_iterative_process = tff.learning.build_federated_averaging_process(\r\n",
        "    model_fn,\r\n",
        "    client_optimizer_fn = lambda: tf.keras.optimizers.Adam(learning_rate = 2e-4),\r\n",
        "    server_optimizer_fn = lambda: tf.keras.optimizers.Adam(learning_rate = 1e-3))"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "4o4vzumvhRJK",
        "outputId": "e46e8fcf-04ee-4a15-d5af-68fc2c188065"
      },
      "source": [
        "str(tmp_iterative_process.initialize.type_signature)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'( -> <model=<trainable=<float32[5,5,1,32],float32[32],float32[5,5,32,64],float32[64],float32[3136,512],float32[512],float32[512,10],float32[10]>,non_trainable=<>>,optimizer_state=<int64,float32[5,5,1,32],float32[32],float32[5,5,32,64],float32[64],float32[3136,512],float32[512],float32[512,10],float32[10],float32[5,5,1,32],float32[32],float32[5,5,32,64],float32[64],float32[3136,512],float32[512],float32[512,10],float32[10]>,delta_aggregate_state=<value_sum_process=<>,weight_sum_process=<>>,model_broadcast_state=<>>@SERVER)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YQJiQArV6NA"
      },
      "source": [
        "del tmp_iterative_process"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_cyo15exIXp"
      },
      "source": [
        "### **IID**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaGCIMLGxMJn"
      },
      "source": [
        "# !rm -rf logs/fit/iid"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VQyWpUBxixB"
      },
      "source": [
        "## New model and optimizers.\r\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\r\n",
        "    model_fn,\r\n",
        "    client_optimizer_fn = lambda: tf.keras.optimizers.Adam(learning_rate = 2e-4),\r\n",
        "    server_optimizer_fn = lambda: tf.keras.optimizers.Adam(learning_rate = 1e-3))\r\n",
        "\r\n",
        "logdir = \"logs/fit/iid\"\r\n",
        "summary_writer = tf.summary.create_file_writer(logdir)\r\n",
        "state = iterative_process.initialize()"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7nTIA7FhRCc",
        "outputId": "99efa2f0-8aa0-44ad-b294-22febaa5b073"
      },
      "source": [
        "%%time\r\n",
        "NUM_ROUNDS = 1_000\r\n",
        "NUM_FRACTION = 10 ## i.e. C=0.1\r\n",
        "\r\n",
        "with summary_writer.as_default():\r\n",
        "    for round_num in range(NUM_ROUNDS):\r\n",
        "        state, metrics = iterative_process.next(\r\n",
        "            state, federated_iid_tr_ds[round_num % NUM_FRACTION::NUM_FRACTION])\r\n",
        "\r\n",
        "        ## Record it.\r\n",
        "        for name, value in metrics[\"train\"].items():\r\n",
        "            tf.summary.scalar(name, value, step = round_num)\r\n",
        "\r\n",
        "        ## Print the metrics every 50 rounds.\r\n",
        "        if not (round_num % 50):\r\n",
        "            print(f\"round {round_num:2d}, metrics = {metrics}\")"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round  0, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.47783333), ('loss', 1.7420615)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 50, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.97283334), ('loss', 0.08015102)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 100, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.989), ('loss', 0.040546186)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 150, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99233335), ('loss', 0.032213673)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 200, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99233335), ('loss', 0.028526966)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 250, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9943333), ('loss', 0.019032685)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 300, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.995), ('loss', 0.024952741)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 350, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99616665), ('loss', 0.018526556)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 400, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9978333), ('loss', 0.00998305)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 450, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9975), ('loss', 0.0128305815)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 500, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99833333), ('loss', 0.0070369923)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 550, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99916667), ('loss', 0.0044726077)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 600, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9985), ('loss', 0.01207945)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 650, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9993333), ('loss', 0.008743548)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 700, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9996667), ('loss', 0.00083032926)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 750, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99983335), ('loss', 0.0012781653)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 800, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 1.0), ('loss', 1.0435566e-06)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 850, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 1.0), ('loss', 4.1204744e-06)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 900, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99983335), ('loss', 0.0020022967)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 950, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 1.0), ('loss', 1.0390652e-06)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "CPU times: user 3h 22min 53s, sys: 36min 25s, total: 3h 59min 19s\n",
            "Wall time: 1h 11min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv-eLWH7x4EP"
      },
      "source": [
        "### **Non-IID**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeYjw-QJEsEo"
      },
      "source": [
        "# !rm -rf logs/fit/non-iid"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj7oxz-Vx4IC"
      },
      "source": [
        "## New model and optimizers.\r\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\r\n",
        "    model_fn,\r\n",
        "    client_optimizer_fn = lambda: tf.keras.optimizers.Adam(learning_rate = 2e-4),\r\n",
        "    server_optimizer_fn = lambda: tf.keras.optimizers.Adam(learning_rate = 1e-3))\r\n",
        "\r\n",
        "logdir = \"logs/fit/non-iid\"\r\n",
        "summary_writer = tf.summary.create_file_writer(logdir)\r\n",
        "state = iterative_process.initialize()"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB0bRwyhx4ID",
        "outputId": "11ae8803-8f18-4ec0-ed07-d1dcd86071b4"
      },
      "source": [
        "%%time\r\n",
        "NUM_ROUNDS = 1_000\r\n",
        "NUM_FRACTION = 10 ## i.e. C=0.1\r\n",
        "\r\n",
        "with summary_writer.as_default():\r\n",
        "    for round_num in range(NUM_ROUNDS):\r\n",
        "        state, metrics = iterative_process.next(\r\n",
        "            state, federated_non_iid_tr_ds[round_num % NUM_FRACTION::NUM_FRACTION])\r\n",
        "\r\n",
        "        ## Record it.\r\n",
        "        for name, value in metrics[\"train\"].items():\r\n",
        "            tf.summary.scalar(name, value, step = round_num)\r\n",
        "\r\n",
        "        ## Print the metrics every 50 rounds.\r\n",
        "        if not (round_num % 50):\r\n",
        "            print(f\"round {round_num:2d}, metrics = {metrics}\")"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:asyncio:Task was destroyed but it is pending!\n",
            "task: <Task pending coro=<ReferenceResolvingExecutor._evaluate() running at /usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py:513> wait_for=<_GatheringFuture finished exception=KeyboardInterrupt()> cb=[Task._wakeup()]>\n",
            "ERROR:asyncio:Task was destroyed but it is pending!\n",
            "task: <Task pending coro=<ReferenceResolvingExecutor._evaluate() running at /usr/local/lib/python3.6/dist-packages/tensorflow_federated/python/core/impl/executors/reference_resolving_executor.py:513> wait_for=<_GatheringFuture pending cb=[Task._wakeup()]> cb=[Task._wakeup()]>\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "round  0, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.716), ('loss', 2.1189778)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 50, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9033333), ('loss', 0.42257154)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 100, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.95033336), ('loss', 0.21872847)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 150, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.96466666), ('loss', 0.15343457)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 200, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.96533334), ('loss', 0.16003492)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 250, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.96783334), ('loss', 0.16230927)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 300, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9715), ('loss', 0.15650277)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 350, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.978), ('loss', 0.1110843)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 400, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9826667), ('loss', 0.08998991)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 450, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9831667), ('loss', 0.09322226)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 500, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9855), ('loss', 0.077376716)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 550, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.98433334), ('loss', 0.10504218)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 600, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.98833334), ('loss', 0.083839394)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 650, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99016666), ('loss', 0.063247174)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 700, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9913333), ('loss', 0.05741885)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 750, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99333334), ('loss', 0.049894385)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 800, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99233335), ('loss', 0.050009582)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 850, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.98933333), ('loss', 0.07815665)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 900, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.9943333), ('loss', 0.041107636)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "round 950, metrics = OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.99516666), ('loss', 0.041597757)])), ('stat', OrderedDict([('num_examples', 6000)]))])\n",
            "CPU times: user 2h 59min 33s, sys: 32min 38s, total: 3h 32min 11s\n",
            "Wall time: 1h 4min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW8M8x4taUFl"
      },
      "source": [
        "## **Displaying Model Metrics in TensorBoard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMjXEMOpyIpU"
      },
      "source": [
        "# %tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jybijAmpyImS",
        "outputId": "cf09171d-cac3-45cd-b326-823cf2a668cd"
      },
      "source": [
        "!tensorboard dev upload --logdir ./logs \\\r\n",
        "    --name \"Simple experiment with MNIST\" \\\r\n",
        "    --description \"Training results from the paper 'Communication-Efficient Learning of Deep Networks from Decentralized Data'\" \\\r\n",
        "    --one_shot"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "***** TensorBoard Uploader *****\n",
            "\n",
            "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
            "the following directory:\n",
            "\n",
            "./logs\n",
            "\n",
            "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
            "data.\n",
            "\n",
            "Your use of this service is subject to Google's Terms of Service\n",
            "<https://policies.google.com/terms> and Privacy Policy\n",
            "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
            "<https://tensorboard.dev/policy/terms/>.\n",
            "\n",
            "This notice will not be shown again while you are logged into the uploader.\n",
            "To log out, run `tensorboard dev auth revoke`.\n",
            "\n",
            "Continue? (yes/NO) yes\n",
            "\n",
            "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&state=FlBjGuvpuDcY5GuZLB3Pr3sp9SoS1E&prompt=consent&access_type=offline\n",
            "Enter the authorization code: 4/1AY0e-g5coIQ8FZTut2CzDbMS1702iKYJ-hAO_nFwdhddTgAAIHbSURq-7jw\n",
            "\n",
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/BmDncAGATNmukm7VdNxR9w/\n",
            "\n",
            "\u001b[1m[2021-02-10T06:25:15]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2021-02-10T06:25:17]\u001b[0m Total uploaded: 4000 scalars, 0 tensors, 0 binary objects\n",
            "\u001b[1m[2021-02-10T06:25:17]\u001b[0m Done scanning logdir.\n",
            "\n",
            "\n",
            "Done. View your TensorBoard at https://tensorboard.dev/experiment/BmDncAGATNmukm7VdNxR9w/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viHHXF6vm8Gt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}